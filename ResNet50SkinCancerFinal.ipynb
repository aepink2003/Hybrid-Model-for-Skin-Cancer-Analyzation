{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 1: Mount**"
      ],
      "metadata": {
        "id": "LFLHLodPKZxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A-qVD0Hsfbm",
        "outputId": "cd242b99-37ce-4c60-bbb2-47fd186d494c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 2: train, test, generate**"
      ],
      "metadata": {
        "id": "xEFM51aDKoNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from PIL import Image\n",
        "from google.colab import drive  # For Google Colab\n",
        "\n",
        "\n",
        "# Define paths (adjust these as needed)\n",
        "BASE_DIR = \"/content/drive/My Drive/cse144-final\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"train/train\")  # Training images in subfolders \"0\", \"1\", ..., \"99\"\n",
        "TEST_DIR = os.path.join(BASE_DIR, \"test/test\")    # Test images (e.g., \"0.jpg\", \"1.jpg\", ...)\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"efficientnetb0_model.pth\")\n",
        "SUBMISSION_CSV = os.path.join(BASE_DIR, \"submission.csv\")\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Define a simple transform for training and validation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset to load images from a folder organized by class folders\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.classes = sorted(os.listdir(root))\n",
        "        self.samples = []\n",
        "        for cls in self.classes:\n",
        "            cls_path = os.path.join(root, cls)\n",
        "            if os.path.isdir(cls_path):\n",
        "                for fname in os.listdir(cls_path):\n",
        "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp')):\n",
        "                        self.samples.append((os.path.join(cls_path, fname), int(cls)))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Custom dataset for test images (assumes all images are in a single folder)\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_dir, transform=None):\n",
        "        self.test_dir = test_dir\n",
        "        self.image_paths = sorted(os.listdir(test_dir), key=lambda x: int(os.path.splitext(x)[0]))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = os.path.join(self.test_dir, self.image_paths[idx])\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.image_paths[idx]\n",
        "\n",
        "# Load the training dataset\n",
        "full_dataset = TrainDataset(TRAIN_DIR, transform=transform)\n",
        "print(\"Total training images:\", len(full_dataset))\n",
        "\n",
        "# Split into training and validation sets (e.g., 80% train, 20% validation)\n",
        "val_split = 0.2\n",
        "num_total = len(full_dataset)\n",
        "num_val = int(num_total * val_split)\n",
        "num_train = num_total - num_val\n",
        "train_dataset, val_dataset = random_split(full_dataset, [num_train, num_val])\n",
        "print(f\"Training images: {num_train}, Validation images: {num_val}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Load EfficientNet-B0 pretrained on ImageNet\n",
        "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Replace the classifier with a custom head for 100 classes\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.7),\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.7),\n",
        "    nn.Linear(512, 100)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = running_val_loss / total_val\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model.eval()\n",
        "\n",
        "# Prepare test dataset and DataLoader\n",
        "test_dataset = TestDataset(TEST_DIR, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# Make predictions on test data and save submission CSV\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for name, pred in zip(img_names, preds.cpu().numpy()):\n",
        "            predictions.append((name, pred))\n",
        "\n",
        "df = pd.DataFrame(predictions, columns=[\"ID\", \"Label\"])\n",
        "df.to_csv(SUBMISSION_CSV, index=False)\n",
        "\n",
        "print(\"Predictions saved as\", SUBMISSION_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCkDt5bqgplo",
        "outputId": "f5b705f3-b88e-4cb5-87e3-5ca42e7890c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Total training images: 1000\n",
            "Training images: 800, Validation images: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 56.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] | Train Loss: 4.6122 Acc: 0.0150 | Val Loss: 4.4805 Acc: 0.0800\n",
            "Epoch [2/50] | Train Loss: 4.3155 Acc: 0.0862 | Val Loss: 4.1554 Acc: 0.0550\n",
            "Epoch [3/50] | Train Loss: 3.8534 Acc: 0.1300 | Val Loss: 3.7863 Acc: 0.0450\n",
            "Epoch [4/50] | Train Loss: 3.3988 Acc: 0.1663 | Val Loss: 3.4374 Acc: 0.1200\n",
            "Epoch [5/50] | Train Loss: 3.0015 Acc: 0.2450 | Val Loss: 3.2017 Acc: 0.1650\n",
            "Epoch [6/50] | Train Loss: 2.6614 Acc: 0.3475 | Val Loss: 3.0228 Acc: 0.1900\n",
            "Epoch [7/50] | Train Loss: 2.3584 Acc: 0.4175 | Val Loss: 2.9693 Acc: 0.2350\n",
            "Epoch [8/50] | Train Loss: 2.0611 Acc: 0.5200 | Val Loss: 2.8225 Acc: 0.2400\n",
            "Epoch [9/50] | Train Loss: 1.8513 Acc: 0.5750 | Val Loss: 2.6391 Acc: 0.3100\n",
            "Epoch [10/50] | Train Loss: 1.6274 Acc: 0.6575 | Val Loss: 2.6392 Acc: 0.2850\n",
            "Epoch [11/50] | Train Loss: 1.4325 Acc: 0.7400 | Val Loss: 2.6214 Acc: 0.3150\n",
            "Epoch [12/50] | Train Loss: 1.3430 Acc: 0.7575 | Val Loss: 2.7450 Acc: 0.3200\n",
            "Epoch [13/50] | Train Loss: 1.1630 Acc: 0.8225 | Val Loss: 2.5640 Acc: 0.3950\n",
            "Epoch [14/50] | Train Loss: 1.0752 Acc: 0.8512 | Val Loss: 2.6275 Acc: 0.3650\n",
            "Epoch [15/50] | Train Loss: 1.0148 Acc: 0.8725 | Val Loss: 2.6508 Acc: 0.3750\n",
            "Epoch [16/50] | Train Loss: 0.9547 Acc: 0.8938 | Val Loss: 2.6443 Acc: 0.3550\n",
            "Epoch [17/50] | Train Loss: 0.9125 Acc: 0.9163 | Val Loss: 2.5012 Acc: 0.3800\n",
            "Epoch [18/50] | Train Loss: 0.8837 Acc: 0.9163 | Val Loss: 2.4095 Acc: 0.4100\n",
            "Epoch [19/50] | Train Loss: 0.8598 Acc: 0.9325 | Val Loss: 2.5390 Acc: 0.4100\n",
            "Epoch [20/50] | Train Loss: 0.8293 Acc: 0.9263 | Val Loss: 2.5216 Acc: 0.3800\n",
            "Epoch [21/50] | Train Loss: 0.8400 Acc: 0.9325 | Val Loss: 2.6357 Acc: 0.3500\n",
            "Epoch [22/50] | Train Loss: 0.8145 Acc: 0.9375 | Val Loss: 2.6925 Acc: 0.3700\n",
            "Epoch [23/50] | Train Loss: 0.8238 Acc: 0.9425 | Val Loss: 2.6552 Acc: 0.4050\n",
            "Epoch [24/50] | Train Loss: 0.7556 Acc: 0.9600 | Val Loss: 2.6114 Acc: 0.3950\n",
            "Epoch [25/50] | Train Loss: 0.7458 Acc: 0.9625 | Val Loss: 2.5491 Acc: 0.4350\n",
            "Epoch [26/50] | Train Loss: 0.7414 Acc: 0.9650 | Val Loss: 2.5021 Acc: 0.4300\n",
            "Epoch [27/50] | Train Loss: 0.7487 Acc: 0.9675 | Val Loss: 2.5482 Acc: 0.4200\n",
            "Epoch [28/50] | Train Loss: 0.7453 Acc: 0.9663 | Val Loss: 2.4993 Acc: 0.4350\n",
            "Epoch [29/50] | Train Loss: 0.7416 Acc: 0.9587 | Val Loss: 2.4876 Acc: 0.4300\n",
            "Epoch [30/50] | Train Loss: 0.7125 Acc: 0.9738 | Val Loss: 2.4493 Acc: 0.4450\n",
            "Epoch [31/50] | Train Loss: 0.6777 Acc: 0.9838 | Val Loss: 2.5163 Acc: 0.4500\n",
            "Epoch [32/50] | Train Loss: 0.7023 Acc: 0.9775 | Val Loss: 2.5474 Acc: 0.4500\n",
            "Epoch [33/50] | Train Loss: 0.6849 Acc: 0.9900 | Val Loss: 2.4797 Acc: 0.4450\n",
            "Epoch [34/50] | Train Loss: 0.6868 Acc: 0.9738 | Val Loss: 2.5277 Acc: 0.4100\n",
            "Epoch [35/50] | Train Loss: 0.6961 Acc: 0.9812 | Val Loss: 2.5953 Acc: 0.4450\n",
            "Epoch [36/50] | Train Loss: 0.6762 Acc: 0.9862 | Val Loss: 2.6441 Acc: 0.4400\n",
            "Epoch [37/50] | Train Loss: 0.6865 Acc: 0.9825 | Val Loss: 2.6903 Acc: 0.4300\n",
            "Epoch [38/50] | Train Loss: 0.6674 Acc: 0.9838 | Val Loss: 2.5845 Acc: 0.4250\n",
            "Epoch [39/50] | Train Loss: 0.6553 Acc: 0.9888 | Val Loss: 2.4471 Acc: 0.4500\n",
            "Epoch [40/50] | Train Loss: 0.6655 Acc: 0.9875 | Val Loss: 2.4313 Acc: 0.4150\n",
            "Epoch [41/50] | Train Loss: 0.6424 Acc: 0.9925 | Val Loss: 2.4746 Acc: 0.4500\n",
            "Epoch [42/50] | Train Loss: 0.6400 Acc: 0.9875 | Val Loss: 2.5326 Acc: 0.4150\n",
            "Epoch [43/50] | Train Loss: 0.6406 Acc: 0.9888 | Val Loss: 2.5316 Acc: 0.4050\n",
            "Epoch [44/50] | Train Loss: 0.6320 Acc: 0.9938 | Val Loss: 2.5307 Acc: 0.4150\n",
            "Epoch [45/50] | Train Loss: 0.6327 Acc: 0.9925 | Val Loss: 2.5297 Acc: 0.4100\n",
            "Epoch [46/50] | Train Loss: 0.6382 Acc: 0.9925 | Val Loss: 2.4611 Acc: 0.4450\n",
            "Epoch [47/50] | Train Loss: 0.6258 Acc: 0.9938 | Val Loss: 2.4206 Acc: 0.4300\n",
            "Epoch [48/50] | Train Loss: 0.6123 Acc: 0.9962 | Val Loss: 2.3847 Acc: 0.4550\n",
            "Epoch [49/50] | Train Loss: 0.6225 Acc: 0.9950 | Val Loss: 2.3319 Acc: 0.4750\n",
            "Epoch [50/50] | Train Loss: 0.6536 Acc: 0.9875 | Val Loss: 2.4426 Acc: 0.4300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e4630a65b726>:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(MODEL_PATH))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved as /content/drive/My Drive/cse144-final/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 3: save csv file**"
      ],
      "metadata": {
        "id": "KguynGf-LCBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_path = \"/content/drive/My Drive/cse144-final/submission.csv\"\n",
        "\n",
        "# Download the file\n",
        "files.download(csv_path)"
      ],
      "metadata": {
        "id": "vmNY-uB97rqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1112b222-79e1-4f94-d302-bd15d5bfc482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e9f7a8ac-03c2-4440-a075-e075bee8ed54\", \"submission.csv\", 10784)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}